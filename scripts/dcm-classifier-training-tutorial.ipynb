{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a1eaa4-2e9c-4eb2-b693-531f2d1fb7b8",
   "metadata": {},
   "source": [
    "# Training Custom Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e629a14-3040-4e91-8325-eaee8ec1352c",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This tutorial is meant to show developers the process for altering the dcm-classifier package for developement with new DICOM data.\n",
    "\n",
    "## Setup\n",
    "\n",
    "If you have not already, clone the git hub repository by running the following in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0408159-2975-4ef2-85ed-34183fe7e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:BRAINSia/dcm-classifier.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2fc1e-08ef-44d0-baa5-9825ee461327",
   "metadata": {},
   "source": [
    "Next, to install the necessary development packages, install the developer requirements using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b4700-3466-407c-9837-7cefa3d71a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements_dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40556d438931ccf",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dcm-classifier==0.6.0rc7 in /home/mbrzus/programming/.venv/dcmdevvenv/lib/python3.10/site-packages (0.6.0rc7)\n"
     ]
    }
   ],
   "source": [
    "# install the dcm-classifier package for tutorail purposes\n",
    "!pip install dcm-classifier==0.6.0rc9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb6a00d-1cdd-464d-8fab-d43ffe0e43c7",
   "metadata": {},
   "source": [
    "## Data Curation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52691ff0-dc1e-4083-86b8-05162e477320",
   "metadata": {},
   "source": [
    "### Field Sheet Creation\n",
    "\n",
    "The first step is to create a DICOM field sheet at a volume level. This code below will generate pandas DataFrame object containing all DICOM tags from the file's header. The `generate_dicom_dataframe` method can be called via basic function call as well as from command line. Here we only generate a small dataframe to display the functions output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a556a2-6cc7-4de1-b713-e834a799423d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:17:34.714492Z",
     "start_time": "2024-03-13T15:17:31.889342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory: /home/mbrzus/programming/xyz/tests/testing_data/anonymized_testing_data/anonymized_data contains 16 DICOM sub-volumes\n",
      "shape: (16, 112)\n",
      "|    |   SeriesNumber | Image Type                                     |   RepetitionTime |   FlipAngle |\n",
      "|---:|---------------:|:-----------------------------------------------|-----------------:|------------:|\n",
      "|  0 |             11 | ['ORIGINAL', 'PRIMARY', 'M', 'NORM', 'DIS2D']  |          4000    |         150 |\n",
      "|  1 |             13 | ['ORIGINAL', 'PRIMARY', 'M', 'NORM', 'DIS2D']  |           446    |         150 |\n",
      "|  2 |              1 | ['ORIGINAL', 'PRIMARY', 'M', 'NORM', 'DIS2D']  |             4.52 |           8 |\n",
      "|  3 |              4 | ['DERIVED', 'PRIMARY', 'MPR', 'NORM', 'DIS2D'] |             4.52 |           8 |\n"
     ]
    }
   ],
   "source": [
    "from create_dicom_fields_sheet import *\n",
    "from pathlib import Path\n",
    "\n",
    "current_directory: Path = Path.cwd()\n",
    "root_directory = current_directory.parent\n",
    "\n",
    "\n",
    "# make the DICOM field sheet based from the anonymized test data\n",
    "dicom_field_sheet: pd.DataFrame = generate_dicom_dataframe(session_dirs=[root_directory.as_posix() + \"/tests/testing_data/anonymized_testing_data/anonymized_data\"],\n",
    "                                                           output_file=\"\",\n",
    "                                                           save_to_excel=False) \n",
    "# show the field sheet shape\n",
    "print(f\"shape: {dicom_field_sheet.shape}\")\n",
    "\n",
    "# we print df head for the first 2 rows only for few selected fields due to size\n",
    "print(dicom_field_sheet.head(4)[[\"SeriesNumber\", \"Image Type\", \"RepetitionTime\", \"FlipAngle\"]].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644fbef6d9c2efe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### To efficiently extract the information run full script in terminal. The full data frame field sheet will be saved as Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06eba56-098d-4f6d-b921-ea0b9dcdf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 create_dicom_fields_sheet.py --dicom_path ../tests/testing_data/anonymized_testing_data/anonymized_data --out ./tutorial_field_sheet.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7939135455291c29",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**Note:** The `create_dicom_fields.py` script can also be automated using the `run_all_dicom_data_sheets.sh` script. The paths for the shell script will need to be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bab27-f7a1-4650-839f-a2e90c0a0ae0",
   "metadata": {},
   "source": [
    "### Field Sheet Combination\n",
    "\n",
    "If you are dealing with multiple field sheets from different datasets, the `combine_excel_spreadsheets.py` script will combine the sheets into one big field sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aab78bba-9e78-47dd-9077-2982ff12fad4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T16:59:29.640177Z",
     "start_time": "2024-03-13T16:59:29.632401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 112)\n",
      "/home/mbrzus/programming/xyz/tests/testing_data/anonymized_testing_data/anonymized_data/11/DICOM/1.3.12.2.1107.5.1.4.3024295249861856527476734919304407350-11-21-1svccb.dcm\n",
      "|    | ImageType                                     |\n",
      "|---:|:----------------------------------------------|\n",
      "|  0 | ['ORIGINAL', 'PRIMARY', 'M', 'NORM', 'DIS2D'] |\n",
      "|  1 | ['ORIGINAL', 'PRIMARY', 'M', 'NORM', 'DIS2D'] |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utilities import combine_all_excel_files\n",
    "\n",
    "# create combined dataframe\n",
    "all_column_names: pd.DataFrame = combine_all_excel_files([dicom_field_sheet])\n",
    "\n",
    "# As we are only using 1 field sheet, this output will be the same as the last\n",
    "print(all_column_names.shape)\n",
    "\n",
    "# The file name for first image in volume\n",
    "print(dicom_field_sheet[\"FileName\"][0])\n",
    "\n",
    "# The image type for the first image in the volume\n",
    "print(dicom_field_sheet[\"ImageType\"].head(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd4578-45e0-40b5-9292-98ff9962d4aa",
   "metadata": {},
   "source": [
    "### Data Pruning\n",
    "\n",
    "In a DICOM study, one imaging session may have multiple files that represent similar data at different times during the scan. This is the case in diffusion data because of the abundance of echo times recorded at scanning. This can cause an inflation in the number of DICOM files that may have the same data in many fields. The `remove_duplicate_rows.py` script allows for the removal of the duplicate rows. Developers can also adjust the features on which they wish to remove duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d17de3fb-a24b-4de9-90d7-0e786cc7a76f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:17:39.662993Z",
     "start_time": "2024-03-13T15:17:39.645258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  (16, 112)\n",
      "|    |   SeriesNumber |\n",
      "|---:|---------------:|\n",
      "|  0 |             11 |\n",
      "|  1 |             13 |\n",
      "|  2 |              1 |\n",
      "|  3 |              4 |\n",
      "|  4 |              7 |\n",
      "|  5 |             15 |\n",
      "|  6 |              5 |\n",
      "|  7 |              5 |\n",
      "|  8 |              6 |\n",
      "|  9 |              9 |\n",
      "| 10 |              3 |\n",
      "| 11 |              2 |\n",
      "| 12 |             10 |\n",
      "| 13 |             12 |\n",
      "| 14 |             14 |\n",
      "| 15 |              8 |\n",
      "\n",
      "Reduced size:  (13, 112)\n",
      "|    |   SeriesNumber |\n",
      "|---:|---------------:|\n",
      "|  0 |             11 |\n",
      "|  1 |             13 |\n",
      "|  2 |              1 |\n",
      "|  3 |              4 |\n",
      "|  4 |              7 |\n",
      "|  5 |             15 |\n",
      "|  6 |              5 |\n",
      "|  8 |              6 |\n",
      "|  9 |              9 |\n",
      "| 12 |             10 |\n",
      "| 13 |             12 |\n",
      "| 14 |             14 |\n",
      "| 15 |              8 |\n"
     ]
    }
   ],
   "source": [
    "from utilities import remove_rows_with_duplicate_values\n",
    "\n",
    "# create slimmed dataframe\n",
    "slimmed_data_frame: pd.DataFrame = remove_rows_with_duplicate_values(input_frame=all_column_names,\n",
    "                                                                     save_to_excel=False)\n",
    "\n",
    "# show the original dataframe size\n",
    "print(\"Original size: \", all_column_names.shape)\n",
    "print(all_column_names[\"SeriesNumber\"].to_markdown())\n",
    "\n",
    "# show the slimmed dataframe which will have 3 rows removed 1 belonging to series number 5 which has 2 subvolumes\n",
    "print(\"\\nReduced size: \", slimmed_data_frame.shape)\n",
    "print(slimmed_data_frame[\"SeriesNumber\"].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb02a627161b5969",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Feature Creation\n",
    "\n",
    "Feature creation is a pertinent step which allows developers to choose the features used in the model. In the `create_training_sheet.py` script, the `parse_column_headers` method allows developers to choose features they believe will be useful to enter into the model.\n",
    "\n",
    "\n",
    "#### Header Dictionary\n",
    "A header dictionary is a spreadsheet with the fields taken from DICOM images or created in the `generate_dicom_dataframe` method. From these fields, you can select whether to keep them or remove them from the training file. You can choose the action for the header by changing the corresponding action, the actions available are \"drop\", \"keep\", \"one_hot_encode_from_array\", and \"one_hot_encode_from_str_col\". Arrays with string values should be one hot encoded as well as columns with only string values. To edit the header dictionary, the `training_config.py` file should be modified.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480616cf1dce5af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:46:32.923751Z",
     "start_time": "2024-03-13T15:46:32.920021Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | header_name                    | action                        |\n",
      "|---:|:-------------------------------|:------------------------------|\n",
      "|  0 | EchoTime                       | drop                          |\n",
      "|  1 | FlipAngle                      | drop                          |\n",
      "|  2 | PixelBandwidth                 | drop                          |\n",
      "|  3 | PixelSpacing                   | drop                          |\n",
      "|  4 | Image Type                     | one_hot_encoding_from_array   |\n",
      "|  5 | Manufacturer                   | one_hot_encoding_from_str_col |\n",
      "|  6 | Diffusion b-value              | drop                          |\n",
      "|  7 | Diffusion Gradient Orientation | drop                          |\n",
      "|  8 | Diffusionb-value               | keep                          |\n",
      "|  9 | Diffusionb-valueMax            | keep                          |\n"
     ]
    }
   ],
   "source": [
    "from training_config import *\n",
    "\n",
    "# show the first 10 rows of the header dataframe\n",
    "print(header_df.head(10).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7592a3ba-5b6e-4c79-8138-a12948ec6068",
   "metadata": {},
   "source": [
    "#### Running the Script\n",
    "\n",
    "In order to utilize the `create_training_sheet.py` script, the header dataframe is needed. The script will then parse the header dictionary and create a training file with the selected features from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b47fc72a-3587-4bca-89d9-65a574052b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-13T15:53:03.324656Z",
     "start_time": "2024-03-13T15:53:03.241151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError: Diffusionb-valueMax\n",
      "(13, 47)\n",
      "|    |   Image Type_ORIGINAL |   Image Type_PRIMARY |   Image Type_M |   Image Type_NORM |\n",
      "|---:|----------------------:|---------------------:|---------------:|------------------:|\n",
      "|  0 |                     1 |                    1 |              1 |                 1 |\n",
      "|    |   Pixel Bandwidth |   Repetition Time |      SAR |   Scanning Sequence_UnknownScanningSequence |   Sequence Variant_UnknownSequenceVariant |\n",
      "|---:|------------------:|------------------:|---------:|--------------------------------------------:|------------------------------------------:|\n",
      "|  0 |               190 |              4000 | 0.499263 |                                           1 |                                         1 |\n"
     ]
    }
   ],
   "source": [
    "from create_training_sheet import parse_column_headers\n",
    "from training_config import *\n",
    "\n",
    "# create the training file\n",
    "training_data_frame: pd.DataFrame = parse_column_headers(header_dataframe=header_df, \n",
    "                                                         input_file=slimmed_data_frame,\n",
    "                                                         save_to_excel=False)\n",
    "\n",
    "# show training file\n",
    "print(training_data_frame.shape)\n",
    "\n",
    "# for the first row, show columns 1-5\n",
    "print(training_data_frame.iloc[0:1, 1:5].to_markdown())\n",
    "\n",
    "# show the 25th to 30th column\n",
    "print(training_data_frame.iloc[0:1, 25:30].to_markdown())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358de1fc-6b75-44f7-8ed3-16cc4e787463",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Labeling the Data\n",
    "\n",
    "**! The developers are responsible to label their new data !**\n",
    "\n",
    "Labeling can be done on the original data sheet or with another sheet. \n",
    "\n",
    "If labeling is done in another sheet, the files can be merged using the `merge_labels_and_training_data` method in the utilities file. The method will merge the label file and the training file on the *FileName* header. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e0ffb-603b-4563-9d8c-1e2d27f98265",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Once the data is prepared, the next step is to train the model. The `modality_classifier_training.py` script is used to train the model. The script provides many methods for easy training of the model, inference, k-fold validation and model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d67c44-0362-4e5b-9c69-d93ed79a2c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
